# Copyright (c) 2023 42dot. All rights reserved.
import torch
import torch.nn.functional as F
from pytorch3d.transforms import matrix_to_euler_angles 

from .loss_util import compute_photometric_loss, compute_masked_loss
from .single_cam_loss import SingleCamLoss


class MultiCamLoss(SingleCamLoss):
    """
    Class for multi-camera(spatio & temporal) loss calculation
    """
    def __init__(self, cfg, rank):
        super(MultiCamLoss, self).__init__(cfg, rank)
    
    def compute_spatio_loss(self, inputs, target_view, cam=None, scale=None, ref_mask=None):
        """
        This function computes spatial loss.
        """        
        # self occlusion mask * overlap region mask
        spatio_mask = ref_mask * target_view[('overlap_mask', 0, scale)]
        loss_args = {
            'pred': target_view[('overlap', 0, scale)],
            'target': inputs['color',0, 0][:,cam, ...]         
        }        
        spatio_loss = compute_photometric_loss(**loss_args)
        
        target_view[('overlap_mask', 0, scale)] = spatio_mask         
        return compute_masked_loss(spatio_loss, spatio_mask) 

    def compute_spatio_tempo_loss(self, inputs, target_view, cam=None, scale=None, ref_mask=None, reproj_loss_mask=None) :
        """
        This function computes spatio-temporal loss.
        """
        spatio_tempo_losses = []
        spatio_tempo_masks = []
        for frame_id in self.frame_ids[1:]:

            pred_mask = ref_mask * target_view[('overlap_mask', frame_id, scale)]
            pred_mask = pred_mask * reproj_loss_mask 
            
            loss_args = {
                'pred': target_view[('overlap', frame_id, scale)],
                'target': inputs['color',0, 0][:,cam, ...]
            } 
            
            spatio_tempo_losses.append(compute_photometric_loss(**loss_args))
            spatio_tempo_masks.append(pred_mask)
        
        # concatenate losses and masks
        spatio_tempo_losses = torch.cat(spatio_tempo_losses, 1)
        spatio_tempo_masks = torch.cat(spatio_tempo_masks, 1)    

        # for the loss, take minimum value between reprojection loss and identity loss(moving object)
        # for the mask, take maximum value between reprojection mask and overlap mask to apply losses on all the True values of masks.
        spatio_tempo_loss, reprojection_loss_min_index = torch.min(spatio_tempo_losses, dim=1, keepdim=True)
        spatio_tempo_mask, _ = torch.max(spatio_tempo_masks.float(), dim=1, keepdim=True)
     
        if hasattr(self,'sptp_recon_con_type'):
            if self.sptp_recon_con_type=='combine':
                assert self.frame_ids[1:]==[-1,1]
                target_view[('stp_reproj_combined', scale)] = \
                    target_view[('overlap', -1, scale)]*(reprojection_loss_min_index==0)*spatio_tempo_mask+ \
                    target_view[('overlap', 1, scale)] * (reprojection_loss_min_index == 1)*spatio_tempo_mask

        return compute_masked_loss(spatio_tempo_loss, spatio_tempo_mask) 
    
    # cvcdepth
    def compute_spatial_depth_consistency_loss(self, inputs, target_view, cam=None, scale=None, ref_mask=None, reproj_loss_mask=None):
        spatio_mask = ref_mask * target_view[('overlap_mask', 0, scale)]
        loss_args = {
            'pred': target_view[('overlap_depth2', 0, scale)],
            'target': target_view[('depth', scale)]
        }
        spatio_depth_consistency_loss = torch.abs(loss_args['pred']-loss_args['target'])

        depth_con_mask = spatio_mask * (loss_args['pred']>0)
        if hasattr(self, 'spatial_depth_consistency_margin'):
            margin = self.spatial_depth_consistency_margin
            depth_con_mask*=spatio_depth_consistency_loss<margin
        
        return compute_masked_loss(spatio_depth_consistency_loss, depth_con_mask)

    # cvcdepth
    def compute_sp_tp_recon_con_loss(self, inputs, target_view, cam=None, scale=None, ref_mask=None, reproj_loss_mask=None):
        sp_tp_recon_con_loss = 0

        for frame_id in self.frame_ids[1:]:
            pred_mask = ref_mask * target_view[('overlap_mask', frame_id, scale)]
            pred_mask = pred_mask * reproj_loss_mask
            pred_mask = pred_mask * target_view[('overlap_mask', 0, scale)]

            loss_args = {
                'pred': target_view[('overlap', frame_id, scale)],
                'target': target_view[('overlap', 0, scale)],
            }
            local_loss = compute_photometric_loss(**loss_args)

            sp_tp_recon_con_loss = sp_tp_recon_con_loss+ compute_masked_loss(local_loss,pred_mask)
            target_view[('sp_tp_recon_con_loss', scale,frame_id)] = local_loss*pred_mask
        
        return sp_tp_recon_con_loss/len(self.frame_ids[1:])


    def compute_pose_con_loss(self, inputs, outputs, cam=None, scale=None, ref_mask=None, reproj_loss_mask=None) :
        """
        This function computes pose consistency loss in "Full surround monodepth from multiple cameras"
        """        
        ref_output = outputs[('cam', 0)]
        ref_ext = inputs['extrinsics'][:, 0, ...]
        ref_ext_inv = inputs['extrinsics_inv'][:, 0, ...]
   
        cur_output = outputs[('cam', cam)]
        cur_ext = inputs['extrinsics'][:, cam, ...]
        cur_ext_inv = inputs['extrinsics_inv'][:, cam, ...] 
        
        trans_loss = 0.
        angle_loss = 0.
     
        for frame_id in self.frame_ids[1:]:
            ref_T = ref_output[('cam_T_cam', 0, frame_id)]
            cur_T = cur_output[('cam_T_cam', 0, frame_id)]    

            cur_T_aligned = ref_ext_inv@cur_ext@cur_T@cur_ext_inv@ref_ext

            ref_ang = matrix_to_euler_angles(ref_T[:,:3,:3], 'XYZ')
            cur_ang = matrix_to_euler_angles(cur_T_aligned[:,:3,:3], 'XYZ')

            ang_diff = torch.norm(ref_ang - cur_ang, p=2, dim=1).mean()
            t_diff = torch.norm(ref_T[:,:3,3] - cur_T_aligned[:,:3,3], p=2, dim=1).mean()

            trans_loss += t_diff
            angle_loss += ang_diff
        
        pose_loss = (trans_loss + 10 * angle_loss) / len(self.frame_ids[1:])
        return pose_loss
    
    def compute_lidar_loss_2d(self, inputs, target_view, cam=0, scale=0, ref_mask=None):

        single_loss = 0
        tempo_loss = 0
        spatio_loss = 0
        spatio_tempo_loss = 0
        
        min_depth, max_depth = 0, 200
        
        for scale in self.scales:
            #print(target_view[('warp_depth', -1, scale)])
            #print(target_view[('warp_depth', 1, scale)])
            #vutils.save_image(target_view[('warp_depth', -1, scale)][0], 'wrap_cur2prev.png')
            #vutils.save_image(target_view[('warp_depth', 1, scale)][0], 'wrap_cur2next.png')
            '''
            wrap1 = target_view[('warp_depth', -1, scale)] * target_view[('warp_depth_mask', -1, scale)]
            plt.imshow(wrap1[0].permute(1, 2, 0).detach().cpu().numpy(), cmap='viridis')
            plt.savefig("wrap_cur2prev.png")
            
            wrap2 = target_view[('warp_depth', 1, scale)] * target_view[('warp_depth_mask', 1, scale)]
            plt.imshow(wrap2[0].permute(1, 2, 0).detach().cpu().numpy(), cmap='viridis')
            plt.savefig("wrap_cur2next.png")
            '''
            # single
            gt_depth = inputs[('gt_depth', 0)][:, cam, :, :]
            pred_depth = target_view[('depth', scale)]
            
            mask = (gt_depth > min_depth) * (gt_depth < max_depth) * inputs['mask'][:, cam, ...]
            mask = mask.bool()
            
            single_loss = F.l1_loss(pred_depth[mask], gt_depth[mask], reduction='mean')
            
            # spatial
            pred_depth = target_view[('overlap_depth', 0, scale)] * target_view[('overlap_depth_mask', 0, scale)]
            spatio_loss = F.l1_loss(pred_depth[mask], gt_depth[mask], reduction='mean')
            
            # temporal & spatial-temporal
            for frame_id in self.frame_ids[1:]:
                pred_depth = target_view[('warp_depth', frame_id, scale)] * target_view[('warp_depth_mask', frame_id, scale)]
                tempo_loss += F.l1_loss(pred_depth[mask], gt_depth[mask], reduction='mean')
                
                pred_depth = target_view[('overlap_depth', frame_id, scale)] * target_view[('overlap_depth_mask', frame_id, scale)]
                spatio_tempo_loss += F.l1_loss(pred_depth[mask], gt_depth[mask], reduction='mean')
            
            tempo_loss = tempo_loss / len(self.frame_ids[1:])
            spatio_tempo_loss = spatio_tempo_loss / len(self.frame_ids[1:])
            
            
        return single_loss / len(self.scales), tempo_loss / len(self.scales), spatio_loss / len(self.scales), spatio_tempo_loss / len(self.scales)


    def forward(self, inputs, outputs, cam):        
        loss_dict = {}
        cam_loss = 0. # loss across the multi-scale
        target_view = outputs[('cam', cam)]
        for scale in self.scales:
            kargs = {
                'cam': cam,
                'scale': scale,
                'ref_mask': inputs['mask'][:,cam,...]
            }
                          
            reprojection_loss = self.compute_reproj_loss(inputs, target_view, **kargs)
            smooth_loss = self.compute_smooth_loss(inputs, target_view, **kargs)
            spatio_loss = self.compute_spatio_loss(inputs, target_view, **kargs)
            
            kargs['reproj_loss_mask'] = target_view[('reproj_mask', scale)]
            spatio_tempo_loss = self.compute_spatio_tempo_loss(inputs, target_view, **kargs)   
            lidar_single_loss, lidar_tempo_loss, lidar_spatio_loss, lidar_spatio_tempo_loss = self.compute_lidar_loss_2d(inputs, target_view, **kargs)

            # pose consistency loss
            if self.pose_model == 'fsm' and cam != 0:
                pose_loss = self.compute_pose_con_loss(inputs, outputs, **kargs)
            else:
                pose_loss = 0
            
            # cvcdepth
            if hasattr(self, 'spatial_depth_consistency_loss_weight'):
                spatial_depth_consistency_loss = self.compute_spatial_depth_consistency_loss(inputs,target_view,**kargs)
            if hasattr(self, 'sp_tp_recon_con_loss_weight'):
                sp_tp_recon_con_loss = self.compute_sp_tp_recon_con_loss(inputs, target_view,**kargs)
            #if hasattr(self, 'spatial_depth_aug_smoothness'):
            #    spatial_depth_aug_smooth_loss = self.compute_spatial_depth_aug_smooth_loss(inputs,target_view,**kargs)


            cam_loss += reprojection_loss
            cam_loss += self.disparity_smoothness * smooth_loss / (2 ** scale)            
            cam_loss += self.spatio_coeff * spatio_loss + self.spatio_tempo_coeff * spatio_tempo_loss                            
            cam_loss += self.pose_loss_coeff* pose_loss
            cam_loss += lidar_single_loss

            # cvcdepth
            if hasattr(self, 'spatial_depth_consistency_loss_weight') :
                cam_loss += self.spatial_depth_consistency_loss_weight * spatial_depth_consistency_loss
            if hasattr(self, 'sp_tp_recon_con_loss_weight'):
                cam_loss += self.sp_tp_recon_con_loss_weight * sp_tp_recon_con_loss
            #if hasattr(self,'spatial_depth_aug_smoothness') :
            #    cam_loss += self.spatial_depth_aug_smoothness * spatial_depth_aug_smooth_loss


            ##########################
            # for logger
            ##########################
            if scale == 0:
                loss_dict['reproj_loss'] = reprojection_loss.item()
                loss_dict['spatio_loss'] = spatio_loss.item()
                loss_dict['spatio_tempo_loss'] = spatio_tempo_loss.item()
                loss_dict['smooth'] = smooth_loss.item()
                loss_dict['lidar_single_loss'] = lidar_single_loss.item()
                if self.pose_model == 'fsm' and cam != 0:
                    loss_dict['pose'] = pose_loss.item()
                
                # cvcdepth
                if hasattr(self, 'spatial_depth_consistency_loss_weight') :
                    loss_dict['spatial_depth_consistency_loss'] = spatial_depth_consistency_loss.item() * self.spatial_depth_consistency_loss_weight
                if hasattr(self, 'sp_tp_recon_con_loss_weight'):
                    loss_dict['sp_tp_recon_con_loss'] = sp_tp_recon_con_loss.item() * self.sp_tp_recon_con_loss_weight
                #if hasattr(self, 'spatial_depth_aug_smoothness'):
                #    loss_dict['spatial_depth_aug_smooth_loss'] = spatial_depth_aug_smooth_loss.item() * self.spatial_depth_aug_smoothness

                # log statistics
                self.get_logs(loss_dict, target_view, cam)                        
        
        cam_loss /= len(self.scales)
        return cam_loss, loss_dict